{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 파노라마(panorama) 영상 만들기\n",
    "## 1) 과정\n",
    "### ① 지역 특징 추출\n",
    "- SIFT(Scale Invariant Feature Transform) 검출기를 사용하여 각각의 영상에서 지역 특징점 검출\n",
    "\n",
    "### ② 대응점 매칭  \n",
    "- ①에서 찾은 지역 특징점 간 매칭 수행\n",
    "- 영상 1의 i번째 특징 $a_i$와 대응되는 영상 2의 j번째 특징 $b_j$를 찾은 후, 매칭 쌍으로 저장함  \n",
    "- 매칭 전략    \n",
    "  - 1) 고정 임계값 사용  \n",
    "    - $d(a_i,b_j) < T$\n",
    "  - 2) 최근접 특징 벡터 탐색\n",
    "  - 3) 최근접 거리 비율 **(실습에서는 이 매칭 방식 사용)**\n",
    "    - $\\frac{d(a_i,b_j)}{d(a_i,b_k)} < T$  \n",
    "    - $a_i$와 거리가 가까운 상위 2개의 대응점 $b_j$와 $b_k$을 찾은 후, 거리 비율을 만족하는 특징만 저장함 (즉, 모호한 대응쌍을 걸러냄)\n",
    "\n",
    "### ③ 변환 행렬($H$) 추정 \n",
    "- 영상 1과 영상 2 사이의 다수의 대응점 쌍으로부터 **RANSAC(RANdom SAmple Consencus)**을 이용하여 변환 행렬 추정  \n",
    "- RANSAC은 이상점(outlier)이 포함된 데이터셋에서 어떠한 모델을 예측할 때 효과적인 방법임\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{equation}  \n",
    "    H = \\begin{bmatrix}  \n",
    "        h_{00} & h_{01} & h_{02} \\\\  \n",
    "        h_{10} & h_{11} & h_{12} \\\\  \n",
    "        h_{20} & h_{21} & h_{22} \\\\  \n",
    "        \\end{bmatrix}  \n",
    "\\end{equation}  \n",
    "$$  \n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "    \\begin{bmatrix}\n",
    "        x' \\\\\n",
    "        y' \\\\\n",
    "        1\n",
    "    \\end{bmatrix} \n",
    "    = H\n",
    "    \\begin{bmatrix}\n",
    "        x \\\\\n",
    "        y \\\\\n",
    "        1\n",
    "    \\end{bmatrix}\n",
    "    =\n",
    "    \\begin{bmatrix}\n",
    "        h_{00} & h_{01} & h_{02} \\\\\n",
    "        h_{10} & h_{11} & h_{12} \\\\\n",
    "        h_{20} & h_{21} & h_{22}\n",
    "    \\end{bmatrix}\n",
    "    \\begin{bmatrix}\n",
    "        x \\\\\n",
    "        y \\\\\n",
    "        1\n",
    "    \\end{bmatrix}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "### ④ 원근 변환(perspective transform ≒ homography) 적용\n",
    "- 추정된 변환 행렬 $H$를 이용하여 영상 2에 원근 변환 적용\n",
    "\n",
    "### ⑤ 두 영상 이어붙이기\n",
    "- 파노라마 결과 영상 = 영상 1 + 변환된 영상 2\n",
    "\n",
    "#  2. Practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(3.4.2) C:\\projects\\opencv-python\\opencv\\modules\\imgproc\\src\\resize.cpp:4044: error: (-215:Assertion failed) !ssize.empty() in function 'cv::resize'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-92cf9f817b7e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m## 테스트할 이미지\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mimg_1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'left.jpg'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mimg_1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mgray_1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCOLOR_BGR2GRAY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mimg_2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'right.jpg'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(3.4.2) C:\\projects\\opencv-python\\opencv\\modules\\imgproc\\src\\resize.cpp:4044: error: (-215:Assertion failed) !ssize.empty() in function 'cv::resize'\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "## 테스트할 이미지\n",
    "img_1 = cv2.imread('left.jpg')\n",
    "img_1 = cv2.resize(img_1, None, fx=0.5, fy=0.5)\n",
    "gray_1 = cv2.cvtColor(img_1,cv2.COLOR_BGR2GRAY)\n",
    "img_2 = cv2.imread('right.jpg')\n",
    "img_2 = cv2.resize(img_2, None, fx=0.5, fy=0.5)\n",
    "gray_2 = cv2.cvtColor(img_2,cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "## 이미지 확인\n",
    "cv2.imshow('image 1', img_1)\n",
    "cv2.imshow('image 2', img_2)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gray_1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-9ff2be6da552>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m## 1) 각각의 영상에서 SIFT 특징 추출\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0msift\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxfeatures2d\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSIFT_create\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m##시프트검출기 생성\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mkp_1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdes_1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msift\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetectAndCompute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgray_1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m##키포인트, 특징기술자\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mkp_2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdes_2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msift\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetectAndCompute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgray_2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'gray_1' is not defined"
     ]
    }
   ],
   "source": [
    "## 1) 각각의 영상에서 SIFT 특징 추출\n",
    "sift = cv2.xfeatures2d.SIFT_create() ##시프트검출기 생성\n",
    "kp_1, des_1 = sift.detectAndCompute(gray_1, None) ##키포인트, 특징기술자\n",
    "kp_2, des_2 = sift.detectAndCompute(gray_2, None)\n",
    "\n",
    "## 2-1) 두 영상의 지역 특징 간 거리 계산\n",
    "## https://docs.opencv.org/2.4/modules/features2d/doc/common_interfaces_of_descriptor_matchers.html#bfmatcher\n",
    "bf = cv2.BFMatcher() ##이미지1의 한점에서 이미지2의 모든점들에 하나하나 매칭 시켜서 비슷한가 보는 기능\n",
    "matches = bf.knnMatch(queryDescriptors = des_1,\n",
    "                      trainDescriptors = des_2,\n",
    "                      k=2)   # 특징점 하나당 거리가 가까운 상위 k개의 대응점을 찾아줌\n",
    "                            ##(점1, 점2) 형식으로 대응점 쌍 리스트를 뱉어줌\n",
    "\n",
    "## 2-2) '최근접 거리 비율' 매칭 전략('ratio testing')을 사용하여 대응점 쌍 생성\n",
    "ratio = 0.7  # 보통 0.7 ~ 0.8 값 사용\n",
    "good = []    # 좋은 대응점 쌍만 따로 저장\n",
    "for m,n in matches:\n",
    "    if m.distance < n.distance * ratio:\n",
    "        good.append(m) ##조건 만족하면 good이라는 리스트에 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "243\n"
     ]
    }
   ],
   "source": [
    "## optional) SIFT 특징점 매칭 결과 확인하기\n",
    "print(len(good)) ##243개의 쌍 중\n",
    "matched = cv2.drawMatches(img1 = img_1, \n",
    "                          keypoints1 = kp_1, \n",
    "                          img2 = img_2, \n",
    "                          keypoints2 = kp_2, \n",
    "                          matches1to2 = good[:20],   # 첫 20개 매칭쌍만 시각화\n",
    "                          outImg = None, \n",
    "                          flags = 2)\n",
    "\n",
    "cv2.imshow('matching result', matched)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3) RANSAC을 이용한 변환 행렬 H 추정\n",
    "## 참고. cv2.getPerspectiveTransform()와 cv2.findHomography()의 차이\n",
    "##   cv2.getPerspectiveTransform() - 4개의 대응점 쌍을 입력하면 변환 행렬을 반환해줌\n",
    "##   cv2.findHomography() - 4개 이상의 대응점 쌍을 입력하면 가장 대응점들을 가장 잘 만족하는 변환 행렬을 반환해줌\n",
    "## https://docs.opencv.org/2.4/modules/calib3d/doc/camera_calibration_and_3d_reconstruction.html#findhomography\n",
    "if len(good) >= 4:   ## 최소 4개 이상의 대응점이 존재해야 원근 변환 행렬을 추정 가능\n",
    "    src_pts = np.float32([ kp_2[m.trainIdx].pt for m in good ]).reshape(-1,1,2) ##소스포인트\n",
    "    dst_pts = np.float32([ kp_1[m.queryIdx].pt for m in good ]).reshape(-1,1,2) ##destination 포인트\n",
    "    H, _ = cv2.findHomography(srcPoints = src_pts, ##호모그라피=원근변환\n",
    "                              dstPoints = dst_pts, \n",
    "                              method = cv2.RANSAC, ##메소드에 란산을 넣어줌\n",
    "                              ransacReprojThreshold = 5) ##몰라도됨\n",
    "    \n",
    "else:\n",
    "    raise AssertionError(\"Can't find enough keypoints.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homework ( ~ 12/02 12:00)\n",
    "- 아래 코드의 '4) 원근 변환 적용', '5) 두 영상을 한 장으로 이어붙이기' 부분을 완성하여 파노라마 영상 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 4) 원근 변환 적용\n",
    "## 변환 행렬 H를 사용하여 변환 수행\n",
    "## https://docs.opencv.org/2.4/modules/imgproc/doc/geometric_transformations.html#warpperspective\n",
    "##오른쪽 영상 기준\n",
    "##cv2.warpPerspective(src, H, 결과영상 사이즈, dst[, flags[, borderMode[, borderValue]]]])\n",
    "\n",
    "res = cv2.warpPerspective(img_2, H, (img_1.shape[1]+img_2.shape[1], img_1.shape[0]))\n",
    "\n",
    "## 5) 두 영상을 한 장으로 이어붙이기\n",
    "##왼쪽영상, 변환된 오른쪽 영상 포개기\n",
    "\n",
    "res[0:img_1.shape[0], 0:img_1.shape[1]] = img_1\n",
    "\n",
    "## 결과 파노라마 영상 시각화\n",
    "cv2.imshow('panorama', res)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
